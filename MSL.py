# -*- coding: utf-8 -*-
"""MSL Sprint 2 - 10% v5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10hkUr_3sXyeq0MB4Atgvlw8Zlqj1zVo6

### School of Innovation Technologies and Engineering

Department of Industrial System Engineering


---



# Interim Report
## Mauritian Sign Language System with Machine Learning

MSC Computer Science with Emerging Technologies

**MSc Dissertation**

**Module code: MCSET23BPT**


Name: Noyan TsurishaddaÃ¯


Student ID: 2310_26618


Academic Year: 2024-2025


Submission Date: 23 March 2025
"""

!pip install keras-tuner

!pip install scikeras

!pip uninstall opencv-python -y
!pip install opencv-contrib-python==4.11.0.86

!pip install mediapipe==0.10.21

# importing libraries and related module
import tensorflow
from tensorflow.keras import layers, models, datasets
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import BatchNormalization, LeakyReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.data import Dataset
from tensorflow.keras.utils import plot_model, to_categorical
from tensorflow.keras.layers import Dropout, Dense, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.models import Sequential

from keras_tuner import RandomSearch

from scikeras.wrappers import KerasClassifier

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split
from sklearn.metrics import classification_report, confusion_matrix

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os
import cv2
from google.colab.patches import cv2_imshow
from PIL import Image # import Image
import mediapipe as mp

"""Step 1: data preprocessing"""

# check if file path exist
file_path = '/content/drive/MyDrive/Colab Notebooks/add_miss_sign_mnist_train.csv'

if not os.path.exists(file_path):
    print(f"Error: Data file '{file_path}' does not exist.")
else:
    print(f"Data file '{file_path}' exists. Loading data...")
    # Reading the sign language dataset as 'asl_data'
    asl_data = pd.read_csv(file_path)
    print("Data loaded successfully.")

# Displaying first 5 in the row
print(asl_data.head())

# Display the shape of the original dataset
print(f"Original dataset has {asl_data.shape[0]} rows and {asl_data.shape[1]} columns.")

dataset_size = asl_data.shape[0]
print(f"Dataset size: {dataset_size}")

# Proper loading and preprocessing of the Sign Language dataset
# separate the features and labels:
# train_images = asl_data.drop(columns=['filename']).iloc[:, 1:].values
train_images = asl_data.iloc[:, 1:].values
train_labels = asl_data.iloc[:, 0].values

# test_images = asl_data.drop(columns=['filename']).iloc[:, 1:].values
test_images = asl_data.iloc[:, 1:].values
test_labels = asl_data.iloc[:, 0].values

# normalise
train_images = train_images / 255.0  # Normalize pixel values to the range [0, 1]
test_images = test_images / 255.0

test_images = test_images / 255.0

import numpy as np
import matplotlib.pyplot as plt

def generate_image(letter1, letter2):
    # Combine pixel values of two letters
    combined_pixels = (letter1 + letter2) / 2
    return combined_pixels

# Example: Generate synthetic image for J (combining I and K)
# Assuming 'label' is the name of the first column containing the label (0-25 for a-z)
# Accessing data for label 8 (i)
i_pixels = asl_data.loc[asl_data['label'] == 8].drop('label', axis=1).values[0]
# Accessing data for label 10 (k)
k_pixels = asl_data.loc[asl_data['label'] == 10].drop('label', axis=1).values[0]
j_pixels = generate_image(i_pixels, k_pixels)

# Reshape to 28x28, ensuring the correct number of elements (784)
# j_pixels = j_pixels.reshape(28, 28) # Changed from j_pixels.reshape(28, 28)


# Display the synthetic image
# plt.imshow(j_pixels, cmap='gray')
# plt.title('Synthetic Image for J')
# plt.show()

import cv2
import numpy as np
import matplotlib.pyplot as plt

def draw_j():
    img = np.ones((28, 28), dtype=np.uint8) * 255  # white background
    # Draw a curved line resembling the letter 'J'
    cv2.polylines(img, [np.array([[14, 10], [12, 15], [14, 20], [16, 25], [18, 20], [16, 15], [14, 10]], dtype=np.int32)], isClosed=False, color=0, thickness=2)
    return img

def draw_z():
    img = np.ones((28, 28), dtype=np.uint8) * 255  # white background
    # Draw a 'Z' shape
    cv2.line(img, (6, 6), (20, 6), 0, 2)  # top horizontal line
    cv2.line(img, (20, 6), (10, 14), 0, 2)  # diagonal line
    cv2.line(img, (10, 14), (6, 22), 0, 2)  # bottom diagonal line
    cv2.line(img, (6, 22), (20, 22), 0, 2)  # bottom horizontal line
    return img

# Generate and display the images
j_img = draw_j()
z_img = draw_z()

import os
import cv2

new_data_dir = "/content/drive/MyDrive/dissertation/MSL/Sprint/Sprint_2/add_missing_alpha.gsheet"

# using the new data directory to add missing alphabets


# for filename in os.listdir(new_data_dir):
#     if filename.endswith(".jpg") or filename.endswith(".png"):
#         img_path = os.path.join(new_data_dir, filename)
#         img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Assuming images are grayscale
        # ... (append img and label to existing data)

# creating dataset entries
data = []
labels = []

# landmark detection
mp_hands = mp.solutions.hands
hands = mp_hands.Hands()

#  creating a list to store theimages and their corresponding labels: label 9 (j) and label 25 (z)

new_images = [j_img, z_img]
new_labels = [9, 25]

# creating a list to store numbers as well as their corresponding labels such as: label 26 (0) till label 35 (9)
# new_images = [img0, img1, img2, img3, img4, img5, img6, img7, img8, img9]
# new_labels = [26, 27, 28, 29, 30, 31, 32, 33, 34, 35]

# Preprocess the images

#  reshape and normalise the images to be compatible with model's format

new_images = np.array(new_images)
new_images = new_images.reshape(-1, 28, 28, 1)
new_images = new_images / 255.0 # Normalize to [0, 1]

# Combining with existing data
train_images = train_images.reshape(-1, 28, 28, 1) # Reshape train_images to have 4 dimensions
train_images = np.concatenate([train_images, new_images])
train_labels = np.concatenate([train_labels, new_labels])

# normalise
train_images = train_images / 255.0  # Normalize pixel values to the range [0, 1]

print(asl_data['label'].unique())

print(asl_data['label'] == 9)

# Check the unique classes in the target variable
num_classes = asl_data['label'].nunique()
print(f'Number of unique classes: {num_classes}')  # Should print 24

# As the dataset is huge, therefore, 100% is selected first to train the data and will gradually progess till 100%
# subset_fraction = 1
# subset_size_train = int(len(train_images)* subset_fraction)
# subset_size_test = int(len(test_images)* subset_fraction)

# train_images, train_labels = train_images[:subset_size_train], train_labels[:subset_size_train]
# test_images, test_labels = test_images[:subset_size_test], test_labels[:subset_size_test]

# Convert train_labels to a consistent data type (e.g., int)
# train_labels = train_labels.astype(int)
# get unique class labels from the training labels
unique_labels = np.unique(train_labels)

# Print the unique class labels
print("Unique class labels in the dataset:")
print(unique_labels)

# define a class name to link the label with the alphabets
class_names = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n',
               'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

# Print the class names
print("Sign Language classes:")
for i, class_name in enumerate(class_names):
    print(f"{i}: {class_name}")

print(asl_data['label'].unique)

print(asl_data[asl_data['label'] == 9].shape)

print(asl_data[asl_data['label'] == 25].shape)

# Count occurrences of each label in the training dataset
unique_labels, counts = np.unique(train_labels, return_counts=True)

# Print the details of unique labels and their counts
print("Label details:")
for label, count in zip(unique_labels, counts):
    print(f'Label {label} ({class_names[label]}): {count} samples')

# adding missing values "J", "Z" images

# collecting images from webcam

# importing dependencies
import cv2
import os
import time
import uuid


from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import numpy as np
import PIL
import io
import html
import time

# creating Image path
img_path = '/content/drive/MyDrive/Colab'

# defining the labels and images to collect

# j, z and 0-9

# setting up an array of labels and number of images to collect

labels = ['j', 'k']
number_imgs = 5

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)

  # get photo data
  data = eval_js('takePhoto({})'.format(quality))
  # get OpenCV format image
  img = js_to_image(data)
  # grayscale img
  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
  print(gray.shape)
  # get face bounding box coordinates using Haar Cascade
  faces = face_cascade.detectMultiScale(gray)
  # draw face bounding box on image
  for (x,y,w,h) in faces:
      img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
  # save image
  cv2.imwrite(filename, img)

  return filename

try:
  filename = take_photo('photo.jpg')
  print('Saved to {}'.format(filename))

  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

# collect images
for label in labels:
  # if label doesn't exist, create a dictionary
  label_dir = os.path.join(img_path, label)
  if not os.path.exists(label_dir):
  #   os.makedirs(label_dir)
    !mkdir {'/content/drive/MyDrive/Colab/' +label}
  # else, the label exist, return true
  os.makedirs(label_dir, exist_ok=True)

  cap = cv2.VideoCapture(0)

  if not cap.isOpened():
      print(f"Error: Could not open camera for {label}.")
      # Skip to the next label if camera cannot be opened
      continue

  print('Collecting images for {}'.format(label))
  time.sleep(5)
  for imgnum in range(number_imgs):
    ret, frame = cap.read()

    # check in frame was successfully captured
    if ret:
      imgname = os.path.join(img_path, label, label+'.'+'{}.jpg'.format(str(uuid.uuid1())))
      cv2.imwrite(imgname, frame)
      cv2.imshow('frame', frame)
      time.sleep(5)

      if cv2.waitKey(1) & 0xFF == ord('q'):
        break
    else:
      print(f"Warning: Could not capture frame {imgnum + 1} for {label}. Skipping.")

  cap.release()
  cv2.destroyAllWindows()

from google.colab import drive
from google.colab.output import eval_js
from IPython.display import display, Javascript, Image, HTML
from base64 import b64decode
import cv2
import numpy as np
import PIL.Image
import io
import os
import time # Import time if you need delays

# Mount Google Drive
drive.mount('/content/drive')

# Helper function to convert JS data URL to OpenCV image (you already have this)
def js_to_image(js_reply):
  """
  Params:
          js_reply: JavaScript object containing image from webcam
  Returns:
          img: OpenCV BGR image
  """
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  # decode numpy array into OpenCV BGR image
  img = cv2.imdecode(jpg_as_np, flags=1)

  return img

# JavaScript code to open camera and provide a capture function
js_camera_setup = Javascript('''
    async function setupCamera() {
      const video = document.createElement('video');
      const liveView = document.getElementById('liveView'); // Assuming you have a div with id 'liveView'
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      liveView.innerHTML = ''; // Clear previous content
      liveView.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Function to capture photo and return data URL
      window.capturePhoto = async function(quality = 0.8) {
        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);
        return canvas.toDataURL('image/jpeg', quality);
      };

       // Function to stop the camera stream
      window.stopCamera = function() {
          const stream = video.srcObject;
          if (stream) {
              stream.getTracks().forEach(track => track.stop());
          }
          liveView.innerHTML = ''; // Clear the video element
      };

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);
    }
''')

# HTML to display controls and live view
camera_ui = HTML('''
<button id="openCameraBtn">Open Camera</button>
<button id="closeCameraBtn" disabled>Close Camera</button>
<div id="liveView"></div>
<script>
    // Enable/disable buttons based on camera state
    document.getElementById('openCameraBtn').onclick = async () => {
        await google.colab.output.evalJs('setupCamera()');
        document.getElementById('openCameraBtn').disabled = true;
        document.getElementById('closeCameraBtn').disabled = false;
    };
    document.getElementById('closeCameraBtn').onclick = () => {
        google.colab.output.evalJs('stopCamera()');
        document.getElementById('openCameraBtn').disabled = false;
        document.getElementById('closeCameraBtn').disabled = true;
    };
</script>
''')


def capture_sign_images_colab(label, num_samples=5):
    """Captures images for a given sign language label using Colab's webcam access."""

    # Google Drive directory path
    drive_dir = '/content/drive/MyDrive/Colab/'  # Adjust this path as needed
    label_dir = os.path.join(drive_dir, label)

    # Create directory if it doesn't exist
    os.makedirs(label_dir, exist_ok=True)

    print(f"Ready to collect {num_samples} images for '{label}'.")
    print("Use the buttons below to control the camera.")
    print(f"After clicking 'Open Camera', position your hand for '{label}' and press 'Capture' when prompted.")


    # Display the camera UI
    display(camera_ui)
    # Display the JavaScript setup code
    display(js_camera_setup)


    # This part requires user interaction via buttons created by the HTML/JS
    # We can't just loop and capture automatically. We need to wait for the user
    # to take a photo.

    # To capture 'num_samples' images, you would typically:
    # 1. Open the camera.
    # 2. Provide a "Capture" button in the UI (can be added to the HTML).
    # 3. When the user clicks "Capture", call the `capturePhoto()` JS function
    #    and get the data URL back in Python.
    # 4. Process and save the image.
    # 5. Repeat steps 3-4 `num_samples` times.
    # 6. Close the camera.

    # Implementing the capture loop with a simple prompt as an example:
    for i in range(num_samples):
        input(f"Press Enter in the Python cell to capture image {i+1}/{num_samples} for '{label}'...")
        try:
            # Call the JavaScript function to capture a photo
            data_url = eval_js('capturePhoto()')

            # Convert the data URL to an OpenCV image
            img = js_to_image(data_url)

            # Process the image (e.g., grayscale, resize if necessary for your model)
            # Assuming your model expects 28x28 grayscale images
            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            resized_img = cv2.resize(gray_img, (28, 28))

            # Save the processed image to Google Drive
            img_path = os.path.join(label_dir, f"{label}_{i}.jpg")
            cv2.imwrite(img_path, resized_img)
            print(f"Image saved: {img_path}")

        except Exception as e:
            print(f"Error capturing image {i+1}/{num_samples}: {e}")

    # After capturing all images, stop the camera
    eval_js('stopCamera()')
    print(f"Finished collecting images for '{label}'. Camera closed.")

# Example usage to capture 5 images for 'j' and 'k':
labels_to_capture = ['j', 'k']
for label in labels_to_capture:
    capture_sign_images_colab(label, num_samples=5)



from google.colab import drive
import cv2
import os

# Mount Google Drive
drive.mount('/content/drive')

def capture_and_save_to_drive(value, num_samples=5):
    """Captures images from camera and saves them to Google Drive."""

    # Google Drive directory path
    img_path = '/content/drive/MyDrive/Colab'
    value_dir = os.path.join(img_path, value)

    # Create directory if it doesn't exist
    os.makedirs(value_dir, exist_ok=True)
    cap = cv2.VideoCapture(0)  # 0 for default camera

    if cap.isOpened():

        print("Opening Camera.")

        return

    if not cap.isOpened():
        print("Error: Could not open camera.")
        return

    count = 0
    while count < num_samples:
        ret, frame = cap.read()

        if ret:
            # Display instructions
            cv2.putText(frame, f"Show sign for '{value}' ({count + 1}/{num_samples})",
                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.imshow('Capture Sign', frame)

            key = cv2.waitKey(1)
            if key == ord('s'):
                img_path = os.path.join(value_dir, f"{value}_{count}.jpg")
                cv2.imwrite(img_path, frame)
                print(f"Image saved: {img_path}")
                count += 1
            elif key == ord('q'):
                break
        else:
            print("Error: Could not capture frame.")
            break

    cap.release()
    cv2.destroyAllWindows()

# Example usage:
missing_values = ['j', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
for value in missing_values:
    capture_and_save_to_drive(value)
    print(f"Images captured for '{value}'")







from google.colab import drive
drive.mount('/content/drive') # Make sure drive is mounted

def capture_and_save_to_drive(value, num_samples=100):
    """Captures images from camera and saves them to Google Drive."""

    # Google Drive directory path
    drive_dir = '/content/drive/MyDrive/Colab Notebooks/captured_images/'  # Adjust the path as needed
    value_dir = os.path.join(drive_dir, value)

    # Create directory if it doesn't exist
    os.makedirs(value_dir, exist_ok=True)

    cap = cv2.VideoCapture(0)  # 0 for default camera

    if not cap.isOpened():
        print("Error: Could not open camera.")
        return

    count = 0
    while count < num_samples:
        ret, frame = cap.read()

        if ret:
            # Display instructions
            cv2.putText(frame, f"Show sign for '{value}' ({count + 1}/{num_samples})",
                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.imshow('Capture Sign', frame)

            key = cv2.waitKey(1)
            if key == ord('s'):
                img_path = os.path.join(value_dir, f"{value}_{count}.jpg")
                cv2.imwrite(img_path, frame)
                print(f"Image saved: {img_path}")
                count += 1
            elif key == ord('q'):
                break
        else:
            print("Error: Could not capture frame.")
            break

    cap.release()
    cv2.destroyAllWindows()

# Example usage to capture 50 new images for 'j' and 'k':
capture_and_save_to_drive('j', num_samples=50)
capture_and_save_to_drive('k', num_samples=50)

# load and processing new images

"""[youtube](https://youtu.be/pDXdlXlaCco?feature=shared)

https://youtu.be/pDXdlXlaCco?feature=shared
"""



# load and preprocess images:
def load_and_preprocess_images(image_paths):
    images = []
    for filename in os.listdir(image_paths):
        if filename.endswith(".jpg") or filename.endswith(".png"):
            img_path = os.path.join(image_paths, filename)
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            img = cv2.resize(img, (28, 28))  # Resize to 28x28
            img = img / 255.0  # Normalize pixel values to [0, 1]
            images.append(img)
    return np.array(images)

# Load images for 'j' (label 9)
j_images = load_and_preprocess_images('/content/drive/MyDrive/Colab Notebooks/j')

z_images = load_and_preprocess_images('/content/drive/MyDrive/Colab Notebooks/z')

# Defining the base model

# Initialize a sequential model to stack layers in a linear format.
model1 = models.Sequential([
    # Reshape the input to (28, 28, 1) to add a channel dimension for grayscale images
    layers.Reshape((28, 28, 1), input_shape=(28, 28)),
    # First convolutional layer with 32 filters of size 3x3, using ReLU activation, and setting the input shape.
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    # First max pooling layer with a pool size of 2x2 to reduce the spatial dimensions.
    layers.MaxPooling2D((2, 2)),
    # Second convolutional layer with 64 filters of size 3x3, using ReLU activation.
    layers.Conv2D(64, (3, 3), activation='relu'),
    # Second max pooling layer with a pool size of 2x2.
    layers.MaxPooling2D((2, 2)),
    # Third convolutional layer with 64 filters of size 3x3, using ReLU activation.
    layers.Conv2D(64, (3, 3), activation='relu'),
    # Flatten the output from the convolutional layers to feed into the dense layers.
    layers.Flatten(),
    # Dense layer with 64 units and ReLU activation.
    layers.Dense(64, activation='relu'),
    # Output layer with 10 units (for the 10 classes) with no activation function specified yet.
    layers.Dense(10)
])

# Generate a plot of the model
plot_model(model1, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)

# Assuming class_names represents the actual labels in your dataset
num_classes = len(class_names)
# Initialize a sequential model to stack layers in a linear format.
model1 = models.Sequential([
    # Reshape the input to (28, 28, 1) to add a channel dimension for grayscale images
    layers.Reshape((28, 28, 1), input_shape=(28, 28)),
    # First convolutional layer with 32 filters of size 3x3, using ReLU activation, and setting the input shape.
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    # First max pooling layer with a pool size of 2x2 to reduce the spatial dimensions.
    layers.MaxPooling2D((2, 2)),
    # Second convolutional layer with 64 filters of size 3x3, using ReLU activation.
    layers.Conv2D(64, (3, 3), activation='relu'),
    # Second max pooling layer with a pool size of 2x2.
    layers.MaxPooling2D((2, 2)),
    # Third convolutional layer with 64 filters of size 3x3, using ReLU activation.
    layers.Conv2D(64, (3, 3), activation='relu'),
    # Flatten the output from the convolutional layers to feed into the dense layers.
    layers.Flatten(),
    # Dense layer with 64 units and ReLU activation.
    layers.Dense(num_classes, activation='relu'),
    # Output layer with 24 units (for the 24 classes)
    layers.Dense(num_classes) # Changed from 10 to 24
])

# Compile the model
model1.compile(optimizer='adam',
              loss=tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# use early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Reshape the training and testing images to (28, 28, 1) before fitting the model
train_images = train_images.reshape(-1, 28, 28, 1)
test_images = test_images.reshape(-1, 28, 28, 1)

# fit the model
# history = model1.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels), callbacks=[early_stopping])

# fit the model
# Ensure your training and validation data is of type float32
train_images = train_images.astype(np.float32)
test_images = test_images.astype(np.float32)
train_labels = train_labels.astype(np.int32)  # or np.int64 if needed
test_labels = test_labels.astype(np.int32)  # or np.int64 if needed

history = model1.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels), callbacks=[early_stopping])

# Model 1
test_loss, test_accuracy = model1.evaluate(test_images, test_labels, verbose=2)
print("Test Accuracy:", test_accuracy)

# Predict th model
y_pred = model1.predict(test_images)
y_pred_classes = np.argmax(y_pred, axis=1)

# model 1
print(classification_report(test_labels, y_pred_classes))

# Model 1
# Plotting for training and validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy  for model 1')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Model 1
# Save the trained model to a file in HDF5 format, allowing easy storage of large numerical data.
model1.save('my_model1.h5')
# Print confirmation that the model was saved successfully.
print("Model saved successfully.")

# model 1
#  Load the model saved in HDF5 format. This model is now ready to be used for inference or further training.
loaded_model1 = tensorflow.keras.models.load_model('my_model1.h5')

# model 1
# Function to plot test images with predictions
def plot_predictions(train_images, unique_labels, train_labels, class_names, num_images=10):
    plt.figure(figsize=(10, 10))
    for i in range(num_images):
        plt.subplot(5, 5, i + 1)
        plt.imshow(train_images[i].reshape(28, 28), cmap='gray')
        plt.axis('off')
        plt.title(f'True: {class_names[unique_labels[i]]}\nPred: {class_names[train_labels[i]]}')
    plt.tight_layout()
    plt.show()

# Plot the first 10 test images with their true and predicted labels
plot_predictions(test_images, test_labels , y_pred_classes,  class_names, num_images=10)

def display_text_as_images(text):
    images_to_display = []
    for char in text.lower():
        if char in class_names:
            # Get the index of the character in class_names
            char_index = class_names.index(char)

            # Find an image in the dataset corresponding to the character
            image_row = asl_data[asl_data['label'] == char_index].iloc[0]  # Get the first matching row
            image_data = image_row.loc['pixel1':'pixel784'].values.reshape(28, 28)  # Reshape to 28x28

            # Convert image data to RGB format for display
            image_rgb = cv2.cvtColor(image_data.astype(np.uint8), cv2.COLOR_GRAY2RGB)
            images_to_display.append(image_rgb)
        else:
            print(f"Character '{char}' not found in the dataset.")

    # Display images side-by-side using matplotlib
    if images_to_display:
        fig, axes = plt.subplots(1, len(images_to_display), figsize=(10, 5))
        for i, img in enumerate(images_to_display):
            axes[i].imshow(img)
            axes[i].set_title(text[i])  # Set title as the original character
            axes[i].axis('off')  # Hide axes
        plt.show()

# Get text input from the user
# text_input = input("Enter text: ")
text_input = "abcdef" # @param {type: "string"}
# Display the text as images
display_text_as_images(text_input)

# function to capture and save images

def capture_and_save(value, num_samples=100):
  # capturing image from camera and saves them with labels

  # Google Drive directory path
  drive_dir = '/content/drive/MyDrive/Colab Notebooks/'
  value_dir = os.path.join(drive_dir, value)

  # creating directory if it does not exist
  if not os.path.exists(value_dir):
    os.makedirs(value_dir)

  cap = cv2.VideoCapture(0) # 0 for default camera
  count = 0

  while count < num_samples:
    ret, frame = cap.read()

    if ret:
      #Display a window with instructions
      cv2.putText(frame, f"Show sign for '{value} ({count + 1}/{num_sample})",
                  (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
      cv2.imshow('Capture Sign', frame)

      key = cv2.waitkey(1)
      if key == ord('s'): # Press 's' to save the image
        img_path = os.path.join(value_dir, f"{value}_{count}.jpg")
        cv2.imwrite(img_path, frame)
        print(f"Image saved: {img_path}")
        count += 1
      elif key == ord('q'): # Press 'q' to quit
        break

  cap.release()
  cv2.destroyAllWindows()

# import dependencies
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import cv2
import numpy as np
import PIL
import io
import html
import time

# function to convert the JavaScript object into an OpenCV image
def js_to_image(js_reply):
  """
  Params:
          js_reply: JavaScript object containing image from webcam
  Returns:
          img: OpenCV BGR image
  """
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  # decode numpy array into OpenCV BGR image
  img = cv2.imdecode(jpg_as_np, flags=1)

  return img

# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream
def bbox_to_bytes(bbox_array):
  """
  Params:
          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.
  Returns:
        bytes: Base64 image byte string
  """
  # convert array into PIL image
  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')
  iobuf = io.BytesIO()
  # format bbox into png for return
  bbox_PIL.save(iobuf, format='png')
  # format return string
  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))

  return bbox_bytes

from IPython.display import display, Javascript, Image, HTML # Added HTML
from google.colab.output import eval_js
def take_photo(filename='photo.jpg', quality=0.8):
  # Create HTML buttons
  buttons_html = """
  <button id="openCameraBtn">Open Camera</button>
  <button id="closeCameraBtn" disabled>Close Camera</button>
  <div id="liveView"></div>
  """
  display(HTML(buttons_html))

  js = Javascript("""
    async function openCamera() {
      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.getElementById('liveView').appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Enable close button, disable open button
      document.getElementById('closeCameraBtn').disabled = false;
      document.getElementById('openCameraBtn').disabled = true;

      // Function to capture photo
      window.capturePhoto = async function() {
        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);
        return canvas.toDataURL('image/jpeg', """ + str(quality) + """);
      }
    }

    function closeCamera() {
      const video = document.getElementById('liveView').querySelector('video');
      if (video) {
        const stream = video.srcObject;
        if (stream) {
          stream.getTracks().forEach(track => track.stop());
        }
        video.remove();
      }
      // Enable open button, disable close button
      document.getElementById('closeCameraBtn').disabled = true;
      document.getElementById('openCameraBtn').disabled = false;
    }

    document.getElementById('openCameraBtn').onclick = openCamera;
    document.getElementById('closeCameraBtn').onclick = closeCamera;
  """)
  display(js)

from google.colab import drive
import cv2
import os

# Mount Google Drive
drive.mount('/content/drive')

def capture_and_save_to_drive(value, num_samples=100):
    """Captures images from camera and saves them to Google Drive."""

    # Google Drive directory path
    drive_dir = '/content/drive/MyDrive/Colab Notebooks/captured_images/'  # Adjust the path as needed
    value_dir = os.path.join(drive_dir, value)

    # Create directory if it doesn't exist
    os.makedirs(value_dir, exist_ok=True)

    cap = cv2.VideoCapture(0)  # 0 for default camera

    if not cap.isOpened():
        print("Error: Could not open camera.")
        return

    count = 0
    while count < num_samples:
        ret, frame = cap.read()

        if ret:
            # Display instructions
            cv2.putText(frame, f"Show sign for '{value}' ({count + 1}/{num_samples})",
                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.imshow('Capture Sign', frame)

            key = cv2.waitKey(1)
            if key == ord('s'):
                img_path = os.path.join(value_dir, f"{value}_{count}.jpg")
                cv2.imwrite(img_path, frame)
                print(f"Image saved: {img_path}")
                count += 1
            elif key == ord('q'):
                break
        else:
            print("Error: Could not capture frame.")
            break

    cap.release()
    cv2.destroyAllWindows()

# Example usage:
missing_values = ['j', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
for value in missing_values:
    capture_and_save_to_drive(value)
    print(f"Images captured for '{value}'")

import cv2

# Initialize the camera
cap = cv2.VideoCapture(0)  # 0 usually refers to the default webcam

# Check if the camera opened successfully
if not cap.isOpened():
    print("Error: Could not open camera.")
    exit()

# Capture a frame
ret, frame = cap.read()

# Check if the frame was captured successfully
if ret:
    # Display the captured frame in a window
    cv2.imshow('Captured Image', frame)

    # Save the captured frame to a file (optional)
    cv2.imwrite('captured_image.jpg', frame)
    print("Image captured and saved to 'captured_image.jpg'")

    # Wait for a key press and then close the window
    cv2.waitKey(0)
    cv2.destroyAllWindows()
else:
    print("Error: Could not capture frame.")

# Release the camera
cap.release()

